{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Noisy Sphere Function\n",
    "\n",
    "Let $f: R^N \\to R$ equal the sphere function; i.e., $$f(x; \\xi)=\\sum_{i=1}^N x_i^2 + \\epsilon (\\xi),$$ where $\\epsilon \\sim U[-k,k]$ is stoachastic additive noise with zero mean and bounded variance. Then we have $\\nabla f(x)=2x$ and one can show that $L_1^*=2$. This means that the value of $||\\nabla f(x) -\\nabla f(y)||$ will never be larger than twice the value of $2||x-y||$ which is obvious because $||\\nabla f(x) -\\nabla f(y)||$ literally equals $2||x-y||$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noisy sphere function evaluated at x_test is 99.99990211852962 and the error-free value is 100\n"
     ]
    }
   ],
   "source": [
    "# Define sphere function with additive noise\n",
    "\n",
    "# Noise level\n",
    "k=1e-4\n",
    "\n",
    "def sphere(x):\n",
    "    x=x**2\n",
    "    return (np.sum(x, axis=0) + k*(2*np.random.rand(1) - 1))[0]\n",
    "\n",
    "F=sphere # rename function as F due to other code\n",
    "\n",
    "N=50 # this is the dimension of the inputs!\n",
    "\n",
    "x_test=np.sqrt(2)*np.ones(N)\n",
    "\n",
    "print('Noisy sphere function evaluated at x_test is', F(x_test), 'and the error-free value is', 2*N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Estimate the standard deviation of the noise, $\\sigma_\\epsilon^2$\n",
    "\n",
    "Since $\\epsilon(\\cdot) \\sim U[-k,k]$, the variance in the rv $\\epsilon(\\cdot)$  is $\\sigma_\\epsilon ^2 = k^2/3$. The below implementation of EC Noise (Chen & More) will form estimators to $\\sigma_\\epsilon^2 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True variance is 3.3333333333333334e-09\n"
     ]
    }
   ],
   "source": [
    "# What's the right answer?\n",
    "\n",
    "correct_var=k**2/3\n",
    "\n",
    "print('True variance is', correct_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize difference table which is M x M\n",
    "# M is number of points to be sampled along some curve and evaluated under f(.)\n",
    "\n",
    "M=6\n",
    "\n",
    "T = np.zeros((M,M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a base point:\n",
    "# ie: the first point on the curve\n",
    "x_b = np.ones((N,1))\n",
    "\n",
    "# Choose a direction to sample in, and normalize it:\n",
    "p_u = np.ones((N,1))\n",
    "p = p_u/np.linalg.norm(p_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form difference table T\n",
    "\n",
    "for i in range(0,M):\n",
    "    T[i,0] = F(x_b + (i/M)*p)\n",
    "\n",
    "for j in range(0,M-1):\n",
    "    for i in range(0,M-j-1):\n",
    "        T[i,j+1] = T[i+1,j] - T[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a row vector to store the k-level estimators (sigma_k^2) \n",
    "# ie: Initialize empty vector for storage, row vector for readability\n",
    "\n",
    "S = np.zeros((1,M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build S according to paper; each k-th component of S is the k-th level estimator for the variance in our noise\n",
    "# which is computed using a scaled average of the k-th level difference values, from the difference table T\n",
    "\n",
    "for i in range(1,M):\n",
    "    S[0,i] = ((np.math.factorial(i)**2.)/np.math.factorial(2*i))*(1./(M-i))*np.sum(T[:,i]**2,axis=0)\n",
    "\n",
    "S=S[:,1:] # Don't need the first column (because first col. of T just holds function values!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.11788671e+00 5.13022761e-04 4.15488266e-09 3.49472777e-09\n",
      "  2.85679839e-09]]\n"
     ]
    }
   ],
   "source": [
    "# Print our estimators\n",
    "\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How did we do?\n",
    "# ie: compute a vector of relative errors between correct variance and k-th level estimator\n",
    "\n",
    "E = np.zeros((1,M-1))\n",
    "for i in range(0,M-1):\n",
    "    E[0,i] = (1./correct_var)*abs(S[0,i] - correct_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3.494727770818714e-09\n"
     ]
    }
   ],
   "source": [
    "# Print the level, k, of the estimator with smallest relative error, and print the error\n",
    "key_index=np.argmin(E)\n",
    "print(key_index, S[0,key_index]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.494727770818714e-09\n"
     ]
    }
   ],
   "source": [
    "# Define our estimated variance as the best one\n",
    "\n",
    "est_var= S[0,key_index]\n",
    "print(est_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Learn the $L_1$ Lipschitz constant\n",
    "\n",
    "We first compute a value $\\epsilon^*$ prescribed in a Callies paper which is a rough estimate of the the maximum contribution of error. For us, 3*(standard deviation) in the error will give an upper bound on 99.7% of error draws, on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00017017497335139256"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std=est_var**0.5\n",
    "eps_star=3*std\n",
    "eps_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a heuristic estimate for $||f''(x_0)||$ and it can be a very rough upper bound -- Chen and More show this experimentally. The following is directly adapted from Algorithm 5.1 in More and Wild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.53909577964365\n"
     ]
    }
   ],
   "source": [
    "# Suggested parameters for an inequality we will test\n",
    "tao_1=100\n",
    "tao_2=0.1\n",
    "\n",
    "# We need a random draw\n",
    "x_0=100*(2*np.random.rand(N,1) - np.ones((N,1)))\n",
    "\n",
    "# Define an arbitrary unit vector\n",
    "unit_v=np.ones(N)/(N**(1/N))\n",
    "\n",
    "# Computes a centered finite difference to approximate f''\n",
    "# Stores all 3 function evaluations\n",
    "def Delta(h):\n",
    "    F_m=F(x_0 - h*unit_v)\n",
    "    F_0=F(x_0)\n",
    "    F_p=F(x_0 + h*unit_v)\n",
    "    return np.array((abs(F_m - 2*F_0 + F_p), F_m, F_0, F_p))\n",
    "\n",
    "# Name some of the things we just made\n",
    "h_a=std**0.25\n",
    "DD=Delta(h_a)\n",
    "D_h_a=DD[0]\n",
    "F_m_a=DD[1]\n",
    "F_0_a=DD[2]\n",
    "F_p_a=DD[3]\n",
    "\n",
    "# Our first candidate for ||f''||\n",
    "mu_a=D_h_a/(h_a**2)\n",
    "\n",
    "# The LHS and RHS from two inequalities we need to test\n",
    "LHS_1=abs(F_p_a-F_0_a)\n",
    "LHS_2=abs(F_m_a-F_0_a)\n",
    "RHS_1=tao_2*max(abs(F_0_a),abs(F_p_a))\n",
    "RHS_2=tao_2*max(abs(F_0_a),abs(F_m_a))\n",
    "\n",
    "# We will use our first guess mu_a, unless some inequalities fail\n",
    "# then we will use mu_b, an alternate guess scaled by mu_a\n",
    "if D_h_a/std>=tao_1 and LHS_1<=RHS_1 and LHS_2<=RHS_2:\n",
    "    mu_f2=mu_a\n",
    "else:\n",
    "    h_b=(std/mu_a)**0.25\n",
    "    mu_f2=Delta(h_b)[0]/h_b**2\n",
    "    \n",
    "print(mu_f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need estimates for the gradients at sample points, which are denoted $\\hat{\\nabla f (x_k)}$. We do so by evaluating the forward difference in each $i$-th component of a sample $x$ to approximate the $i$-th partial derivative of $f$ evaluated at $x$, $$\\frac{\\partial f}{\\partial x_i}(x)\\approx \\frac{f(x+ h^* \\cdot e_i)-f(x)}{h^*} \\quad h^*:=8^{1/4}\\left(\\frac{\\sigma}{\\mu_{f''}}\\right)^{1/2} \\quad \\mu_{f''} \\approx \\max |f''| \\quad e_i:=(0,\\ldots,0,1,0,\\ldots,0).$$ In our case, $\\mu_{f''}=2$ since the Hessian of $f$, $\\nabla^2 f$ is a $10 \\times 10$ diagonal matrix with entries of 2 along the diagonal, and we are using the standard Euclidean norm. Note that the Frobenius norm equals $\\sqrt{40}$.\n",
    "\n",
    "Chen and More show that $h^*$ yields the best estimates to the partial derivates in an $\\mathcal{L}_1$ sense.\n",
    "\n",
    "With a pairs of estimates $||\\nabla f(x_k)||, ||\\nabla f(x_{k+1})||$ we form the ratios $$\\frac{||\\nabla f(x_k)-\\nabla f(x_{k+1})||-2\\epsilon^*}{x_k-x_{k+1}}$$ and take the maximum such ratio as a numerical estimate to $L_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=6\n",
    "\n",
    "x_vals=np.zeros((N,1)) # matrix to store the vectors we randomly make\n",
    "\n",
    "grads=np.zeros((N,1)) # matrix to store the gradients we approximate\n",
    "\n",
    "ratios=np.zeros((M,1)) # vector storing the ratios we are intersted in!\n",
    "\n",
    "h_star=(8**0.25)*np.sqrt(std/mu_f2) # the prescribed step size by Chen/More\n",
    "\n",
    "for j in range(0,M):\n",
    "\n",
    "    # Take two draws inside a large hypercube and store them\n",
    "    x=100*(2*np.random.rand(N,1) - np.ones((N,1)))\n",
    "    y=100*(2*np.random.rand(N,1) - np.ones((N,1)))\n",
    "    x_vals=np.hstack((x_vals,x))\n",
    "    x_vals=np.hstack((x_vals,y))\n",
    "\n",
    "    # For loop to make finite diff approx's \n",
    "    # to each partial and form approx gradient at x\n",
    "    approx_grad_x=np.zeros((N,1))\n",
    "    for i in range(0,N):\n",
    "        e = np.zeros((N,1))\n",
    "        e[i] = 1.0\n",
    "        approx_grad_x[i] = (F(x + h_star*e) - F(x))/h_star\n",
    "    \n",
    "    # Store the gradient at x\n",
    "    grads=np.hstack((grads,approx_grad_x))\n",
    "    \n",
    "    # For loop to make finite diff approx's \n",
    "    # to each partial and form approx gradient at y\n",
    "    approx_grad_y=np.zeros((N,1))\n",
    "    for p in range(0,N):\n",
    "        e = np.zeros((N,1))\n",
    "        e[p] = 1.0\n",
    "        approx_grad_y[p] = (F(y + h_star*e) - F(y))/h_star\n",
    "    \n",
    "    # Store the gradient at y\n",
    "    grads=np.hstack((grads,approx_grad_y))\n",
    "    \n",
    "    # Form ratios to estimate L_1\n",
    "    # Note we subtract by 2*eps_star, which was mentioned in Callies paper\n",
    "    diff_1=np.linalg.norm(approx_grad_x - approx_grad_y) - 2*eps_star\n",
    "    diff_2=np.linalg.norm(x-y)\n",
    "    r=diff_1/diff_2\n",
    "    \n",
    "    ratios[j]=r\n",
    "    \n",
    "x_vals=x_vals[:,1:]\n",
    "grads=grads[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the ratios are [[1.99998943 1.99980899 1.99984239 1.99989853 1.99998135 1.99988401]] and our estimate to L_1 is 1.9999894280766328\n"
     ]
    }
   ],
   "source": [
    "L_1_est=(np.max(ratios))\n",
    "\n",
    "print('the ratios are', np.transpose(ratios),'and our estimate to L_1 is', L_1_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have now obtained estimates to both $\\sigma_\\epsilon^2$ and $L_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: Perform STARs (with no bells and whistles)\n",
    "\n",
    "Chen and Wild define the optimal hyperparameters $$\\mu^* := \\left(\\frac{8\\sigma_\\epsilon^2 N}{L_1^2(N+6)^3}\\right)^{1/4} \\quad \\quad h:= \\frac{1}{4L_1(N+4),}$$ where $\\mu^*$ as a smoothing factor and $h$ is a step size. We will use our estimates to $\\sigma_\\epsilon^2$ and $L_1$ to compute the hyperparameters needed for STARs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0011877183474750595 0.00231482705090191\n"
     ]
    }
   ],
   "source": [
    "mu_star=((8*est_var*N)/(L_1_est**2*(N+6)**3))**0.25\n",
    "h=1/(4*L_1_est*(N+4))\n",
    "\n",
    "print(mu_star,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def STARS(x_init,F, mu_star,h):\n",
    "    \n",
    "    # x_init: initial x value\n",
    "    # F: function we wish to minimize\n",
    "    # mu_star: smoothing parameter\n",
    "    # h: step length\n",
    "    \n",
    "    # Evaluate noisy F(x_init)\n",
    "    f = F(x_init)\n",
    "    \n",
    "    # Draw a random vector of same size as x_init\n",
    "    u = np.random.normal(0,1,(N,1))\n",
    "    \n",
    "    # Form vector y, which is a random walk away from x_init\n",
    "    y = x_init + (mu_star)*u\n",
    "    \n",
    "    # Evaluate noisy F(y)\n",
    "    g = F(y)\n",
    "    \n",
    "    # Form finite-difference \"gradient oracle\"\n",
    "    s = ((g - f)/mu_star)*u \n",
    "    \n",
    "    # Take descent step in direction of -s smooth by h to get next iterate, x_1\n",
    "    x = x_init - (h)*s\n",
    "   \n",
    "    # Evaluate noisy F(x_1)\n",
    "    f=F(x)\n",
    "    \n",
    "        \n",
    "    return [x, f, y, g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1) 169042.2838376563\n"
     ]
    }
   ],
   "source": [
    "# Draw a single x_init to seed all experiments with same value below\n",
    "\n",
    "x_init=100*(2*np.random.rand(N,1)-np.ones((N,1)))\n",
    "\n",
    "print(np.shape(x_init),F(x_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16791887437724354 0.028222529492468926\n"
     ]
    }
   ],
   "source": [
    "# Run pure STARs\n",
    "\n",
    "x=x_init\n",
    "f=F(x)\n",
    "\n",
    "x_hist=np.array(x)\n",
    "f_hist=np.array(f)\n",
    "\n",
    "maxit=2000\n",
    "\n",
    "for i in range(1, maxit):\n",
    "    s=STARS(x,F,mu_star,h)\n",
    "    x=s[0]\n",
    "    f=s[1]\n",
    "    x_hist=np.hstack((x_hist,x))\n",
    "    f_hist=np.hstack((f_hist,f))\n",
    "    \n",
    "print(np.linalg.norm(x),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEACAYAAAC6d6FnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl41eWd9/H392wJWQiQhSUBQtiD4gJSxbpQKaIVbWtbpculrZXaq3R1rql92pn2acf2aWeeXq0dppYqDzPTEcbSVtHSMlaLuKBlUZRFMESEsCYBEpKQ/X7+SMCISUg4Oef3O+d8XtfFJec+v+XL3TSfc9/37/c75pxDRERST8DrAkRExBsKABGRFKUAEBFJUQoAEZEUpQAQEUlRCgARkRSlABARSVEKABGRFKUAEBFJUQoAEZEUFfK6gN7k5eW54uJir8sQEUkYmzdvrnLO5fdlW18HQHFxMZs2bfK6DBGRhGFmb/d1W19OAZnZAjNbWlNT43UpIiJJy5cB4Jx7wjm3KCcnx+tSRESSli8DQEREYk8BICKSohQAIiIpypcBoEVgEZHYMz9/JeTUCy92yx9/usf3zayX93o+bi9v9bpfb3uadbxrZp3/BcPOHK/r63f9/cw5rW/H6PLeO1XZu+oACAWMoRkRAoFe/0EikmTMbLNzbmZftvX1fQDlVfXctvQlr8tIWMGAkZcVoSA7nYLsNAoGp5Hf+fchGWGy08Nkp4cYnB468/dB4WCvwSoiycPXATAuL5Nln39ft+/1Nm7pbVDjetmz9/16O1/nUV3H8Z3rOJbr8p7r3OCd9i7bdm7H2e1nHaPr8burq2t7c2sbVXXNHKlt5OjJJg7WNLK14gTV9c29/juz00OMGZZx5s/oYRnkZ6e9Z2SSFgqSlR4iKy1IVlqY9HCgY7QS6BjFBKxj5BLoDJOur8+MbhQ0Ip7ydQBkpYWYPSHP6zKSSktbO9V1zdQ2tnCysYXaxlZONrZ2/P1UK4drTrHvWAO7j5zk6TeO0tzaHtN6Ap1BEDhriux0UAQ658XeGyCnX3fsFzAIBQOMGpLO2GGZjMnNID8rjewuo5us9FDniCdMWiigAJKUF7cAMLNrgR8A24GVzrl18Tq3vCMcDDAiJ50ROenn3La93XH0ZBPV9U1nRg3trmME0tTaTl1TC3VNbdQ1ttLY0vbOaKVzpNLeOYpp79y5vb1jJHP6GKdHNqdft79r9NPN/me2c+86V9eaKo438PQbR6iqaz5HPxhZaSHSQkHSwgHSQgHSQkEyIkHys9PIy0ojEgqQEQmSnR5mUDhIejjAoHCQoqEZlORnkpnm689PIucU1U+wmS0DbgKOOucu6NI+H/g5EAQecs79HzpmK+qAdKAimvNKfAQC1uew8Ju6plaO1zefGd2cbGylrqn1XaOeuqYWmlvbaWptP/PfuqZWth+spepkE81tHW3dSQsFWHR1CVNGDGbqyGzSwkEKstMIB315YZ1It6L9CLMc+FfgP043mFkQWAJ8kI5f9BvNbDXwnHPuWTMbDvwU+FSU5xbpUVZaiKwB+ITe2tYRCo0t7TS2tFHX1ErF8VOs3LiPXzxT9q5ts9NDfOUDE5k1bhjTi3I0xSS+F9X/Q5xz682s+KzmWUCZc64cwMxWArc453Z0vn8cSIvmvCLxEgoGGJIReVfbBYU5zL9gBDWnWnjzyEkOnDhFQ3Mbj7y8j/vX7ASgJC+T8QVZ5GZGzqxxZEQ6po+K8zKZPT5XowXxXCwmMQuB/V1eVwDvM7OPAtcDQ+gYNXTLzBYBiwDGjBkTg/JEBkbOoDAzi4dx+oLrT8wczdvV9bz81jHWvH6It6rqea3ixJm1jJONLWemlIIBY2hGmK9cN5FJw7PJy4oQ7gybwekhjR4kLmIRAN395Drn3O+B359rZ+fcUjM7BCyIRCIzBrw6kRgJBoyS/CxK8rNYOOu9H17a2x3V9c1s3X+CLfuOs25XJf/4+PZujzUsM8JnLh/LzOKhTBuVw5BBYd3UJwMu6juBO6eAnjy9CGxmVwDfc85d3/n6WwDOuR/199gzZ850+kIYSVatbe3srW7gcE0j1fVNtLY5quubqGtsZfO+47y4p/pd92zkZ6cxsSCLK0pyuW7qcCYNzyKkaSQ5S3/uBI5FAISA3cB1wAFgI/BJ51z3H3W6P+YCYMGECRPufvPNN6OqTyRRVdU1sevwSXYeqqW2seMeja37a9h99CTOdSx0f/bKYr40ZwLp4aDX5YpPxC0AzGwFcC2QBxwBvuuce9jMbgR+RsdloMucc/efz/E1AhB5r6O1jTz3ZhWrNlewobyaycOzuWZyPlNGZHPT9FFEQhoVpLK4jgBiQSMAkb55ascRfvrUbsqOnqSlzZGdHmLO5AKmjhzMtZPzmTpysNclSpwlfACcphGASN8453hqxxF+t6WC1ytqOFjTCMAHphRw5YQ8PjlrDIMimiZKBQkfABoBiETnwIlTfP+J7bx5pI7yqnqGZoR56I7LmDF2qNelSYwlfACcphGASPT+8EoF//jYdjLTQqz56lUMy4yceydJWP0JAF+uFukbwUQGzkcuKWLFoss5Vt/Mxx98kX3VDV6XJD7hywBwzj3hnFuUk5PjdSkiSeGCwhz+7ycuYk9lPR/5txd4YutBquqavC5LPKbn2YqkiAUXjaLmVAv/9McdfHnFKwQM7phdzK2XFjEmN4PB6WGvS5Q48+UagBaBRWKnsaWNTXuP88Azb/K3t46daZ80PItFV49n3rThCoMEpkVgEemT8so6th2sZf+xBpa/uJfKk00MyQhz15XjuPvqEt1hnIAUACLSb02tbWzee5xfPruH596sYsqIbL42dxLXTxuup5MmkIQPAE0BiXjroefK+eGanbQ7mD9tBLfNGs2cyQVelyV9kPABcJpGACLeOdHQzJce2cKGPdW0O7iiJJefL7yYguzE+4rQVKIAEJEBc6q5jf96+W1+8uddAFwzOZ9/+FApY3IzPK5MutOfANBloCLSq0GRIJ+/qoSZxcP43eYKHvnbPl7aU83n3j+OL147XgvFCUwjABHpl7/uOsoPntxBeWU9g8JBbrxwJJ+/apyePOoTCT8FpEVgEX9rb3c8+foh1m4/zJrXD+EcXD0pn7lTC7jxwpHkZaV5XWLKSvgAOE0jABH/O3DiFEv+WsYLZVW8Xd1AKGDcNH0kX7luIiX5WV6Xl3IUACLiiW0Hali6vpzVWw8CsOeHNxLUl9nHVcI/DVREEtMFhTk8sPASFs+ZAMDPn9YUrp8pAERkwH3jg5MYkhHmtYoTXpcivYhrAJhZppltNrOb4nleEYmvQMCYM7mAdbsq2bCn2utypAdRBYCZLTOzo2a27az2+Wa2y8zKzOy+Lm99E3g0mnOKSGL4+/mTyU4L8YX/3ERdU6vX5Ug3oh0BLAfmd20wsyCwBLgBKAUWmlmpmc0FdgBHojyniCSAkTmD+PHHplPb2MrX//tVhYAPRRUAzrn1wLGzmmcBZc65cudcM7ASuAWYA1wOfBK428y0/iCS5OZPG8Gs4mE8teMIV/zoaVZtrvC6JOkiFo+CKAT2d3ldAbzPObcYwMzuBKqcc+3d7Wxmi4BFAGPGjIlBeSISL4GA8ZvPv48t+47z1ZWv8He/3Uo4aNxycaHXpQmxWQTu7qLfMzcbOOeWO+ee7Gln59xS59xM59zM/Pz8GJQnIvEUCQW4vCSXZ+69lgsKB/PVla9y5//7G3/edoj2dv/eh5QKYhEAFcDoLq+LgIP9OYCZLTCzpTU1NQNamIh4JzMtxG+/MJvPXTmOTXuPc89vtvDtx7ade0eJmVgEwEZgopmNM7MIcDuwOgbnEZEEMygS5B8XlLL1u/P41PvGsHLjPpb8tQw/P5EgmUV7GegKYAMw2cwqzOwu51wrsBhYC+wEHnXObe/PcZ1zTzjnFuXk5ERTnoj4VDBg3DtvMpeNHcY/r93FUzt0caAXfPksID0NVCQ1tLa1c9n9fyESCvCTj13ENZO07hethH8WkEYAIqkhFAzw0B0zaWhq445lf2P5C295XVJK8WUAaBFYJHXMGDuM9X8/hzHDMvjeEzv43urtWhOIE18GgEYAIqllaGaEJ778fm69tIjlL+7V84PixJcBoBGASOrJGRTmuzeXMjQjzP1rdmoUEAe+DACNAERS0+D0MF+bO4ntB2v55u9e48CJU16XlNR8GQAikrpunVHEh6aP5NFNFdz8i+c1HRRDvgwATQGJpK6stBBLPnkpf/7aVYSDARb++iUe0DeLxYQvA0BTQCIyZcRg/nLvNUwbNZifPrWbZ944omcHDTBfBoCICLwzGsiIBPnc8k28/8fPsOb1Q16XlTQUACLia8V5mTxz77V8d0EpzW3tLH5kC4+/esDrspKCLwNAawAi0tWInHQ+e+U41n7tasblZfLVla+ybtdRr8tKeL4MAK0BiEh3crPSeOxLVzI2N4N7H91KTUOL1yUlNF8GgIhIT7LTw3z7xqlU1zcz72fPsv9Yg9clJSwFgIgknHnTRvAfn5vFycZWblnyAicbNRI4HwoAEUlIV0/KZ8knL+VYfTO3/eolTjW3eV1SwvFlAGgRWET6Ys6UAr51wxR2HKrlD6/oyqD+8mUAaBFYRPpq0dUlzBg7lJ/9ZTcNza1el5NQfBkAIiJ9ZWb8rxuncPRkE9/83evUNykE+koBICIJb8bYYdz7wUk8+dpB/m1dmdflJIy4BYCZTTWzB81slZl9MV7nFZHU8OXrJjJt1GB+uW6PnhnUR1EFgJktM7OjZrbtrPb5ZrbLzMrM7D4A59xO59w9wCeAPn1hsYhIf1xfOoJ2B8tf3Ot1KQkh2hHAcmB+1wYzCwJLgBuAUmChmZV2vncz8DzwdJTnFRF5jy/NmUBJfibff3IHL+6p8roc34sqAJxz64FjZzXPAsqcc+XOuWZgJXBL5/arnXOzgU9Fc14Rke4EAsa/LryUguw0vvHfW2lq1b0BvYnFGkAhsL/L6wqg0MyuNbMHzOxXwJqedjazRWa2ycw2VVZWxqA8EUlmpaMGs/gDEzhc28iKl/d5XY6vhWJwTOumzTnn1gHrzrWzc24psBRg5syZWskRkX679dIiHt20n+8/uYO91Q3cO28S2elhr8vynViMACqA0V1eFwEH+3MA3QksItHITAvxyN2X8+GLC1n+4l6+89i2c++UgmIRABuBiWY2zswiwO3A6hicR0SkR4PTw/z0tou5/bLRPP7qQfZW1Xtdku9EexnoCmADMNnMKszsLudcK7AYWAvsBB51zm3vz3H1KAgRGSiLri4B4OHn3/K4Ev+Jag3AObewh/Y19LLQey5mtgBYMGHChPM9hIgIACX5WXxo+kgee+UAfzdvMjkZWgs4zZePgtAIQEQG0mdnF9PY2sbHf/WiHhvdhS8DQIvAIjKQZhYPY+lnZrL7SB1f/K/NtLS1e12SL/gyADQCEJGBNmdKAT/48AWs21XJb1562+tyfMGXAaARgIjEwmcuH0tJfibP7tZNpuDTANAIQERi5eqJ+azbVcnTO494XYrnfBkAIiKx8vW5k8jPTuP+P+6kLcUfG+3LANAUkIjESk5GmLuvGkd5VT2LH9lCc2vqLgj7MgA0BSQisXT3VSV8fe4k/rTtcEovCPsyAEREYsnM+Orcjm8Qe+i5cmpOtXhdkicUACKSsr5941QO1jTyz2vf8LoUT/gyALQGICLxMHtCHjdeOIIVf9tPbWPqjQJ8GQBaAxCReLlz9jja2h0//Z/dXpcSd74MABGReJk1bhjzp41g+Yt72XX4pNflxJUCQERS3vdunkZGJMgPntzhdSlxpQAQkZQ3IiedO2YXs6G8mpqG1FkL8GUAaBFYROJtXulw2todq7ce8LqUuPFlAGgRWETi7eLRQ7iseCg/+fMuyivrvC4nLnwZACIi8WZm/Oz2SwiHAnzhPzdT19TqdUkxpwAQEelUOGQQv1h4CW8ereMfHtvmdTkxpwAQEeniygl5XFGSy3NvVtKe5E8LjVsAmNmHzezXZva4mc2L13lFRPrrhgtHUFXXzG9eTu4HxUUVAGa2zMyOmtm2s9rnm9kuMyszs/sAnHOPOefuBu4EbovmvCIisbRw1hjystL40Zo3OFbf7HU5MRPtCGA5ML9rg5kFgSXADUApsNDMSrts8p3O90VEfCkcDPAvH5/OqZY21u066nU5MRNVADjn1gPHzmqeBZQ558qdc83ASuAW6/Bj4E/OuS3RnFdEJNaunphP4ZBB/OGV5L0vIBZrAIXA/i6vKzrbvgzMBT5mZvf0tLOZLTKzTWa2qbJSX9wsIt4IBIxbLy3khbIqDtc0el1OTMQiAKybNuece8A5N8M5d49z7sGednbOLQX+N7AlEonEoDwRkb75yKVFtDt47NXkHAXEIgAqgNFdXhcBB2NwHhGRmBqXl8mMsUP5/ZYKr0uJiVgEwEZgopmNM7MIcDuwuj8H0KMgRMQvbrhgBLuP1LF2+2GvSxlw0V4GugLYAEw2swozu8s51wosBtYCO4FHnXPb+3lcPQxORHxh4awxFA4ZxNL15V6XMuCivQpooXNupHMu7Jwrcs493Nm+xjk3yTk33jl3/3kcVyMAEfGFzLQQn7liLJvfPs5bVfVelzOgfPkoCI0ARMRPbpo+EoDHk2wx2JcBoBGAiPhJ0dAM5k4tYNnzb3Gquc3rcgaMLwNAIwAR8ZtPXz6W2sZWXiir8rqUAePLANAIQET85vKSXCLBAE+/ccTrUgaMLwNARMRv0sNBrr9gBKs2V3C0NjnuDPZlAGgKSET86J5rSmhpc/xuS3IsBvsyADQFJCJ+NG1UDhcW5rB0/Z6k+MpIXwaAiIhffWPeJI43tPDDNTu9LiVqCgARkX6YM7mAm6aPZPWrB2lsSexLQn0ZAFoDEBE/u+XiQuqaWtl+MLF/R/kyALQGICJ+NmZYBgA7DtZ6XEl0fBkAIiJ+NrEgi0nDs1iV4FcDKQBERPopEDA+MXM0W/efSOhpIAWAiMh5uPXSIsJBY/kLe70u5bz5MgC0CCwifjc0M8Jtl43m8VcPUtPQ4nU558WXAaBFYBFJBLdfNobmtnYe35qYawG+DAARkUQwbdRgLirK4aHn3qK93XldTr8pAEREzpOZccfsYvYda+CZN456XU6/KQBERKJw0/RRZESCPLUj8R4THbcAMLMSM3vYzFbF65wiIrEWCQWYNW4Yz+6upC3BpoGiCgAzW2ZmR81s21nt881sl5mVmdl9AM65cufcXdGcT0TEj+ZMLuBwbSNlR+u8LqVfoh0BLAfmd20wsyCwBLgBKAUWmllplOcREfGtKyfkAfDUjsMeV9I/UQWAc249cOys5llAWecn/mZgJXBLNOcREfGzCQVZjM/P5JV9J7wupV9isQZQCOzv8roCKDSzXDN7ELjEzL7V085mtsjMNpnZpsrKyhiUJyIy8CYNz2bXkZNel9EvoRgc07ppc865auCec+3snFtqZoeABZFIZMaAVyciEgMTh2fzp22HaWhuJSMSi1+tAy8WI4AKYHSX10XAwf4cQHcCi0iiKR2ZDcDOQ4nziOhYBMBGYKKZjTOzCHA7sLo/B9CzgEQk0VxRkkc4aPx5W+IsBEd7GegKYAMw2cwqzOwu51wrsBhYC+wEHnXObY++VBER/8rJCHPNpHzWvJ4iAeCcW+icG+mcCzvnipxzD3e2r3HOTXLOjXfO3X8ex9UUkIgknPdPyOPAiVPsP9bgdSl94stHQWgKSEQS0cziYQC8uKfK40r6xpcBoBGAiCSiySOyyU4P8acEWQfwZQCIiCSicDDAnbOLWberkm0H/D+D4csA0BSQiCSqhbPGAPDyW2c/JMF/fBkAmgISkUQ1MiedrLQQb1fXe13KOfkyADQCEJFEZWZMHpHNjoP+vyHMlwGgEYCIJLLZ43PZvO84x+qbvS6lV74MABGRRDZnSgHOwfrd/n6gpQJARGSATS/MITczwlM7/f01kb4MAK0BiEgiCwUDXDe1gPU+/5pIXwaA1gBEJNGVjhzMycZWTjT4dx3AlwEgIpLocjLCANQ2tnpcSc8UACIiMTAsMw2AiuP+fTCcAkBEJAYuGDUYgA17qj2upGe+DAAtAotIosvNSuOi0UP4zUtv+3Yh2JcBoEVgEUkGt80cTW1jK+WVdV6X0i1fBoCISDKYWTwUgNd9+mRQBYCISIyMz89iUDjIaxUKABGRlBIMGNNGDWbLvuNel9KtuAWAmWWa2b+b2a/N7FPxOq+IiJdmT8jjtYoaahpavC7lPaIKADNbZmZHzWzbWe3zzWyXmZWZ2X2dzR8FVjnn7gZujua8IiKJYnphx8UsG8r9dzlotCOA5cD8rg1mFgSWADcApcBCMysFioD9nZu1RXleEZGEcNWkPDIiQVZu3Od1Ke8RVQA459YDZ3/v2SygzDlX7pxrBlYCtwAVdIRA1OcVEUkUaaEgM8YO5fWKGpzz1/0AsfhFXMg7n/Sh4xd/IfB74FYz+yXwRE87m9kiM9tkZpsqK/39LG0Rkb6YO3U41fXN7Dvmr8dChGJwTOumzTnn6oHPnmtn59xSMzsELIhEIjMGvDoRkTibXtSxDvC3t44xNjfT42reEYsRQAUwusvrIuBgfw6gO4FFJJlcVDSE3MyI7xaCYxEAG4GJZjbOzCLA7cDq/hxAzwISkWQSCBiXl+TyQlmVr9YBor0MdAWwAZhsZhVmdpdzrhVYDKwFdgKPOue2R1+qiEjiumJ8Lkdqm3hl/wmvSzkjqjUA59zCHtrXAGuiOO4TwBMzZ868+3yPISLiJx+YUgDAtgM1XDpmqMfVdPDl5ZiaAhKRZDMyJ50hGWF2Hqr1upQzfBkAWgQWkWRjZkwans3W/f75YOvLABARSUZXTchjx6FaquqavC4F8GkAaApIRJLRrHHDANjytj+eDurLANAUkIgko5L8LAAO1TR6XEkHXwaAiEgyys2MkBEJ+uYbwnwZAJoCEpFkFAgYH7pwJKs2V3Cy0fvvB/BlAGgKSESS1UcuKQTg8Vf79YScmPBlAIiIJKsrxucyYnA6m/ae/ST9+PNlAGgKSESSlZkxcXgW5VX1XpfizwDQFJCIJLPSkYPZeaiWWo/XAXwZACIiyeyqifm0tDnP7wdQAIiIxNmlY4cwKBxktccLwQoAEZE4y4iEWDhrDI9vPcix+mbP6vBlAGgRWESS3dypBbS1O7Z6+P0AvgwALQKLSLIrHTUYgKffOOJZDb4MABGRZDckI8KFhTm8VuHdTIcCQETEI9OLcth/rMGz8ysAREQ8UjQ0g+MNLdQ1tXpy/rgFgJmVmNnDZrYqXucUEfGzCws71jmf3VXpyfn7FABmtszMjprZtrPa55vZLjMrM7P7ejuGc67cOXdXNMWKiCSTK8bnUpCdxsqN+zw5f19HAMuB+V0bzCwILAFuAEqBhWZWamYXmtmTZ/0pGNCqRUSSQDBgfGBKATsOevNF8aG+bOScW29mxWc1zwLKnHPlAGa2ErjFOfcj4KaBLFJEJFkV52VSXd9MbWMLg9PDcT13NGsAhcD+Lq8rOtu6ZWa5ZvYgcImZfauX7RaZ2SYz21RZ6c28mIhIvJTkZQLwxqGTcT93n0YAPbBu2lxPGzvnqoF7znVQ59xSMzsELIhEIjOiqE9ExPcuH59LOGj88bWDZ740Pl6iGQFUAKO7vC4CBuTJRroTWERSxeD0MDdNH8VvN1fQ2NIW13NHEwAbgYlmNs7MIsDtwOqBKErPAhKRVDKvdDgNzW3sPhLfaaC+Xga6AtgATDazCjO7yznXCiwG1gI7gUedc9sHoiiNAEQklYwvyALg9QPx/dDb16uAFvbQvgZYM6AV0TECABZMmDBhoA8tIuI7Y3MzANge58tBffkoCI0ARCSVpIWClORn8tgrB+J6Xl8GgNYARCTVjM/Poqm1nfb2Hi+mHHC+DACNAEQk1dx44Qja2h0vlVfH7Zy+DACNAEQk1Vw/bQThoPEv/7Mrbuf0ZQBoBCAiqSYjEmJETjo743hHsC8DQEQkFb1/Qj6nWtpoao3PDWG+DABNAYlIKhqf3/FcoOffrIrL+XwZAJoCEpFU9OnLxwKwLk5fEOPLABARSUXp4SAfLB3Os7tTOAA0BSQiqco52HesgeP1zTE/ly8DQFNAIpKqPn35GK6dnM+gSDDm54rm+wBERGSAXTu5gGsnx+dbdH05AhARkdhTAIiIpChfBoAWgUVEYs+XAaBFYBGR2PNlAIiISOwpAEREUpQCQEQkRSkARERSlDkXv68f6y8zqwROAGdfDpTTh7Y8ID6P1Ov+/LHcvy/bnmubnt7vS99215bM/d3X7XvbRv3t3/7urr277eLZ59H091jnXH6ftnTO+foPsPR82oBNXtcZq/37su25tunpffV3bPpc/e3f/u6hf7v73yBufR5tf/f1TyJMAT0RRVs8RXv+/uzfl23PtU1P76u/z3/73rZRfw/89gPV3921J3p/94mvp4CiYWabnHMzva4jVai/40v9HX/J2OeJMAI4X0u9LiDFqL/jS/0df0nX50k7AhARkd4l8whARER6oQAQEUlRCgARkRSVEgFgZplm9u9m9msz+5TX9aQCMysxs4fNbJXXtaQCM/tw58/342Y2z+t6kp2ZTTWzB81slZl90et6zlfCBoCZLTOzo2a27az2+Wa2y8zKzOy+zuaPAqucc3cDN8e92CTRnz53zpU75+7yptLk0M/+fqzz5/tO4DYPyk14/ezvnc65e4BPAAl7aWjCBgCwHJjftcHMgsAS4AagFFhoZqVAEbC/c7O2ONaYbJbT9z6X6C2n//39nc73pf+W04/+NrObgeeBp+Nb5sBJ2ABwzq0Hjp3VPAso6/z02QysBG4BKugIAUjgf7PX+tnnEqX+9Ld1+DHwJ+fclnjXmgz6+/PtnFvtnJsNJOy0crL9MizknU/60PGLvxD4PXCrmf0S72/xTjbd9rmZ5ZrZg8AlZvYtb0pLSj39jH8ZmAt8zMzu8aKwJNXTz/e1ZvaAmf0KWONNadELeV3AALNu2pxzrh74bLyLSRE99Xk1oF9EA6+n/n4AeCCgoR/0AAAArklEQVTexaSAnvp7HbAuvqUMvGQbAVQAo7u8LgIOelRLqlCfx5f6O76Sur+TLQA2AhPNbJyZRYDbgdUe15Ts1Ofxpf6Or6Tu74QNADNbAWwAJptZhZnd5ZxrBRYDa4GdwKPOue1e1plM1Ofxpf6Or1Tsbz0MTkQkRSXsCEBERKKjABARSVEKABGRFKUAEBFJUQoAEZEUpQAQEUlRCgARkRSlABARSVEKABGRFPX/AQrnn+rAhBU2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.loglog(f_hist)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

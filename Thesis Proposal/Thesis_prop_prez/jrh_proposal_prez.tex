\documentclass[11pt]{beamer}


\usepackage{amssymb, amsmath, graphicx, caption, enumerate}
\graphicspath{ {images/} }
\usepackage{amsthm}
\usepackage{xargs}
\usepackage{scalerel}




\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\ds}{\displaystyle}
\newcommand{\op}[1]{\left(#1\right)}
\newcommand{\cp}[1]{\left[#1\right]}
\newcommand{\av}[1]{\left| #1\right|}
\newcommand{\st}[1]{\left\{#1\right\}}


\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\newcommandx{\question}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
\newcommandx{\add}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}
\newcommandx{\improve}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}
\newcommandx{\thiswillnotshow}[2][1=]{\todo[disable,#1]{#2}}
\newcommandx{\remove}[2][1=]{\todo[linecolor=yelllow,backgroundcolor=yellow!10,bordercolor=red,#1]{#2}}


\newcommand\reallywidehat[1]{\arraycolsep=0pt\relax%
\begin{array}{c}
\stretchto{
  \scaleto{
    \scalerel*[\widthof{\ensuremath{#1}}]{\kern-.5pt\bigwedge\kern-.5pt}
    {\rule[-\textheight/2]{1ex}{\textheight}} %WIDTH-LIMITED BIG WEDGE
  }{\textheight} % 
}{0.5ex}\\           % THIS SQUEEZES THE WEDGE TO 0.5ex HEIGHT
#1\\                 % THIS STACKS THE WEDGE ATOP THE ARGUMENT
\rule{-1ex}{0ex}
\end{array}
}


\usetheme{CUDenver}
\renewcommand{\familydefault}{\sfdefault}
\usefonttheme[onlymath]{serif}
\usepackage{tikz}
\usetikzlibrary{arrows}
\setbeamertemplate{itemize items}{>>}
\setbeamertemplate{itemize subitem}[square]
\setbeamertemplate{itemize subsubitem}[ball]
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}



\author{Jordan R. Hall}

\title[CUDenver Theme]{Thesis Proposal}

\institute[UCD]{
Department of Mathematical and Statistical Sciences\\
University of Colorado Denver
}

\date{Tuesday, December 4, 2018}

\begin{document}
% ------------------------------------------------


\begin{frame}[t,plain]
    \titlepage
\end{frame}

% start the content of the presentation

\begin{frame}{Overview}
\tableofcontents
\end{frame}


% ------------------------------------------------
\section{Literature Review and Framework}
% ------------------------------------------------

\begin{frame}

\begin{center}
\textbf{Notation}
\end{center}

\begin{itemize}

	\item We define a model parameter space $\Lambda$ with dimension $N$ and a data space $\mathcal{D}$ with dimension $M$. 
	\begin{itemize}
		\item In most settings, $\Lambda$ will be of higher dimension than $\mathcal{D}$.
	\end{itemize}
	
	\item We define a parameter-to-data map $f:\Lambda\rightarrow \mathcal{D}$, 
	\begin{itemize} 
	 	\item $f$ may be polluted by noise.
		\item  $\nabla f$ may be inaccessible.	
	\end{itemize}
	
	\item We write $d=f(\lambda) \in \mathcal{D}$ to denote a particular datum corresponding to the evaluation of a point $\lambda \in \Lambda$. %\alphax
	
	%\item We note a probability distribution function of a random variable x as

\end{itemize}

\end{frame}

% ------------------------------------------------
\subsection{Data-Consistent Inversion (DCI)}
% ------------------------------------------------

\begin{frame}

	\begin{block}{The Stocastic Inverse Problem(SIP)\footnotemark[1]\footnotemark[2]\footnotemark[3]}
	
	\begin{itemize}

	%\item Following Butler, Stuart, and Tarantola, we formulate the Stochastic Inverse Problem (SIP).
	
	\item  %: We define prior knowledge of the parameter space, given by t
	The \emph{prior distribution}, $\pi_\Lambda^\text{prior}(\lambda)$:
	Our prior or initial knowledge of the parameter space $\Lambda$.

	\item %We define 
	The \emph{observed distribution}, $\pi_\mathcal{D}(d)$:  
	our uncertain state of knowledge of the observed data $\mathcal{D}$.
	
	\item The solution of our SIP, %The SIP is the task of finding 
	an \emph{updated probability distribution}, $\pi_\Lambda^\text{update}$: which
	combines the given prior information and the observed data.
	
\end{itemize}

\end{block}

\footnotetext[1]{T. Butler and J. Jakeman and T. Wildey.
``Combining Push-Forward Measures and Bayes' Rule to Construct Consistent Solutions to Stochastic Inverse Problems." SIAM Journal on Scientific Computing, Volume 40, No. 2, pp. A984-A1011, 2018.}
	\footnotetext[2]{Tarantola, Albert. ``Inverse Problem Theory and Methods for Model Parameter Estimation." SIAM. 2005.}
	\footnotetext[3]{Stuart, Andrew. ``Inverse problems: A Bayesian perspective." Acta Numerica, volume 19, pp. 451-559. 2010}
	
\end{frame}


\begin{frame}
\begin{itemize}

\item Given a distribution on $\lambda \in \Lambda$, the \emph{forward Uncertainty Quantification (UQ) problem} is finding the probability distribution of $f(\lambda)$. The forward UQ problem is, in its own right, a nontrivial and important problem in UQ.

\item The \textit{classical Bayesian} or \textit{statistical Bayesian} solution to the inverse problem is generally {\em not consistent}:  
\textit{pull-back probability measure}, meaning the image of the updated distribution $\pi_\Lambda^\text{update}$ under the map $f$, called the \textit{push-forward} of $\pi_\Lambda^\text{update}$, is not equal to the observed probability distribution, $\pi_\mathcal{D}$. 


\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}

	\item Data-consistent inversion (DCI) seeks an updated solution $\pi_\Lambda^\text{update}$ for which the push-forward exactly equals $\pi_\mathcal{D}$. 
	
	\item To obtain such a solution, we must solve the forward problem $f(\pi_\Lambda^\text{prior})$.
	
	\item We denote the solution to the forward problem with $\pi_\mathcal{D}^{f(\Lambda)}(d)$.

\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}


	\item As in Butler, the data-consistent solution to the SIP is 
	
\begin{equation} \label{eq:1}
\pi_\Lambda^\text{update}=\pi_\Lambda^\text{prior}(\lambda)\frac{\pi_\mathcal{D}(f(\lambda))}{\pi_\mathcal{D}^{f(\Lambda)}(f(\lambda))}.
\end{equation}

	
\end{itemize}


\end{frame}

% ------------------------------------------------
\subsection{Optimization Methods for Solving Inverse Problems}
% ------------------------------------------------

\begin{frame}

\begin{itemize}

	\item With an expensive $f$ and large $N$, approximately solving the forward UQ problem needed to form \eqref{eq:1} generally requires density estimation which converges at best near a Monte Carlo $\mathcal{O}\left(\frac{1}{\sqrt{N}} \right)$ convergence rate.
	
	\item The solution to the SIP can be obtained exactly or approximately by solving an equivalent deterministic convex optimization problem.
	
	\begin{itemize}
		\item The classical formulation of the objective function (Tarantola) can be altered (Wildey, Butler, et al) to obtain a data-consistent (approximate) solution, depending on $f, \pi_\Lambda^{\text{prior}},$ and $\pi_\mathcal{D}(d)$.
	\end{itemize}

\end{itemize}

\end{frame}


\begin{frame}

\begin{itemize}
	
	\item Given observed data $d_{\text{obs}} \sim \pi_\mathcal{D}$ and a draw $\lambda_\text{prior}\sim \pi_\Lambda^\text{prior}$, as in Tarantola, we define the \textit{classical misfit function,}


\begin{equation} \label{eq:2}
S(\lambda)=\frac{1}{2}\left(\left|\left|C_\mathcal{D}^{-1/2}(f(\lambda)-d_{\text{obs}})\right|\right|_2^2+\left|\left|C_\Lambda^{-1/2}(\lambda-\lambda_{\text{prior}})\right|\right|_2^2\right).
\end{equation} 

\item In Tarantola, it is assumed that $\pi_\Lambda^\text{prior}$ and $\pi_\mathcal{D}$ are Gaussians, and the misfit function is specified by $\lambda_{\text{prior}}=\bar{\lambda}$ and $d_{\text{obs}}=\bar{d}$, the respective means of the prior and observed densities.

\item The minimum of $S$ is called the maximum a posteriori point, or \textit{MAP point}, $\lambda_S^*$, and the classical solution to the SIP is $\pi_\Lambda^\text{update}(\lambda) \sim \exp(-S(\lambda_S^*)).$ 


\end{itemize}


\end{frame}



\begin{frame}

\begin{itemize}

	\item Widley, Butler, et al reformulate $S$ to write an objective function with a minimizer which is a data-consistent MAP point. We write the \textit{data-consistent misfit function,}
	
	
	
$$T(\lambda)=\frac{1}{2}\left(\left|\left|C_\mathcal{D}^{-1/2}(f(\lambda)-d_{\text{obs}})\right|\right|_2^2+\left|\left|C_\Lambda^{-1/2}(\lambda-\lambda_{\text{prior}})\right|\right|_2^2\right)$$
	

\begin{equation} \label{eq:3}
-\frac{1}{2}\left(\left|\left|C_A^{-1/2}(f(\lambda)-f(\lambda_{\text{prior}}))\right|\right|_2^2\right),
\end{equation} 

\noindent where $C_A=AC_\Lambda A^T$. 
	
	\item An additional ``deregularization" term is appended so that if a unique solution exists, the regularization will be ``turned off." For now, we assume that $f$ is linear (or can be linearized locally), and write the matrix $A$ to define that action of $f$ on $\Lambda$. 





\end{itemize}

\end{frame}


\begin{frame}



\begin{itemize}

	\item We note that in the case that $\pi_\Lambda$ and $\pi_\mathcal{D}$ are Gaussian and $f$ is linear, the data-consistent solution to our inverse problem is given exactly by $\pi_\Lambda^\text{update}(\lambda)\sim \exp(-T(\lambda))$. 

	\item The deregularization term will ensure a solution that updates the distribution on $\Lambda$ only in the directions in which the data is informative. 



\end{itemize}



\end{frame}




% ------------------------------------------------
\section{Research Questions}
% ------------------------------------------------
\begin{frame}

$x$

\end{frame}



% ------------------------------------------------

\section{Preliminary Results and Research Plan}
% ------------------------------------------------
\begin{frame}

$x$

\end{frame}




% ------------------------------------------------
\section{Timeline}
% ------------------------------------------------
\begin{frame}

We outline a rough timeline for the remaining 2 years in a 5.5 year plan.

\begin{itemize}
\tiny

\item Clean existing algorithms and examples, generate richer research results related to DFO and active subspaces, and build a model inverse problem for investigation. (Dec 2018/Jan 2019)

\item On RA for Spring 2019.

\item Write and present MS-level results. (Feb/Mar 2019)

\item Work on theoretical formulation of deregularization for nonlinear $f$. Work on generating notebooks and examples. (Ongoing/Spring and Summer 2019)

\item Summer research; begin writing thesis; summer school/internship/conference (?) (Jun/Jul/Aug 2019)

\item Fall 2019 - Writing phase, revisions.

\item Spring 2020 - Final revisions, software.

\item Summer 2020 Internship/collaborations, publishing, formatting.

\item Defend thesis Summer 2020 or early Fall 2020.


\end{itemize}


\end{frame}



% ------------------------------------------------

\section{References}
% ------------------------------------------------
\begin{frame}

$x$

\end{frame}







\end{document}

